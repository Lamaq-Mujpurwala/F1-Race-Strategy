# This workflow automates the DVC data pipeline efficiently for incremental updates.
name: "Automated Data Pipeline"

on:
  # Allows you to run this workflow manually from the Actions tab on GitHub
  workflow_dispatch:
  
  # Triggers the workflow on push events to the main branch
  push:
    branches:
      - main

jobs:
  run-data-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: pip install -r requirements-data.txt

      - name: Configure DagsHub DVC Remote
        env:
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc remote modify origin --local auth basic
          dvc remote modify origin --local user ${{ secrets.DAGSHUB_USERNAME }}
          dvc remote modify origin --local password $DAGSHUB_TOKEN
      
      - name: Pull Existing Data from DVC Remote
        run: |
          # This synchronizes the workspace with the last successful run
          dvc pull

      # --- THE FIX ---
      - name: Prepare for Incremental Update
        run: |
          # Dynamically get the current year (e.g., 2025)
          CURRENT_YEAR=$(date +'%Y')
          FILE_TO_DELETE="data/raw/laps_${CURRENT_YEAR}.csv"
          
          echo "Preparing for new data fetch for year ${CURRENT_YEAR}."
          echo "Checking for and removing old file: ${FILE_TO_DELETE}"
          
          # Remove the current year's raw data file if it exists.
          # This forces the 'extract' stage of 'dvc repro' to run again for the current year.
          # The '-f' flag ensures the command doesn't fail if the file doesn't exist.
          rm -f $FILE_TO_DELETE

      - name: Run DVC Pipeline to Process New Data
        run: |
          # DVC will now see that the input for the 'extract' stage is missing/changed
          # and will re-run it and all downstream stages (combine, transform).
          dvc repro

      - name: Push New Data to DVC Remote
        run: |
          # This pushes only the new/changed data artifacts to DagsHub
          dvc push -r origin

      - name: Commit and Push DVC Lock File
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add dvc.lock params.yaml processed_races.json
          # The following command checks if there are any changes to commit
          git diff --staged --quiet || git commit -m "Update DVC lockfile and params after data pipeline run"
          git push
