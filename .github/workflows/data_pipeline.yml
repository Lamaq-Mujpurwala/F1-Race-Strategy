# This workflow automates the DVC data pipeline.
name: "Automated Data Pipeline"

on:
  # Allows you to run this workflow manually from the Actions tab on GitHub
  workflow_dispatch:
  
  # Triggers the workflow on push events to the main branch
  push:
    branches:
      - main

jobs:
  run-data-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Dependencies
        run: pip install -r requirements-data.txt

      - name: Configure DagsHub DVC Remote
        # This step uses secrets stored in your GitHub repository
        env:
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # The DVC remote URL is read from the existing .dvc/config file
          # We only need to configure the credentials
          dvc remote modify origin --local auth basic
          dvc remote modify origin --local user ${{ secrets.DAGSHUB_USERNAME }}
          dvc remote modify origin --local password $DAGSHUB_TOKEN
      
      - name: Run DVC Pipeline
        run: |
          # This command executes all stages in dvc.yaml
          dvc repro

      - name: Push Data to DVC Remote
        run: |
          # This pushes the new/changed data artifacts to DagsHub
          dvc push -r origin

      - name: Commit and Push DVC Lock File
        # This is a crucial step. After 'dvc repro', the dvc.lock file is updated.
        # We must commit this file back to Git to keep our project state consistent.
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add dvc.lock params.yaml processed_races.json
          # The following command checks if there are any changes to commit
          git diff --staged --quiet || git commit -m "Update DVC lockfile and params after data pipeline run"
          git push
